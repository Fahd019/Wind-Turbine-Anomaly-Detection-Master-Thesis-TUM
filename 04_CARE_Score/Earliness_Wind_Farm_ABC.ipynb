{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9f2775-c3b5-44b7-8379-24a933497a71",
   "metadata": {},
   "source": [
    "## Earliness Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d397db1-8997-4223-b223-58f28aea4249",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a816a5-6e2e-4868-8dd0-56fa30ff9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02174846-e013-42ba-ac52-a7286acce038",
   "metadata": {},
   "source": [
    "## Wind Farm A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b260ef6-2ee4-40f1-b1ce-c59a718511b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 âœ¨ Weighted Score (WS) for Event [52436â€“54447]: 0.4361\n",
      "Dataset 10 âœ¨ Weighted Score (WS) for Event [52611â€“53591]: 0.1181\n",
      "Dataset 22 âœ¨ Weighted Score (WS) for Event [51888â€“52892]: 0.2090\n",
      "Dataset 26 âœ¨ Weighted Score (WS) for Event [52261â€“53269]: 0.0388\n",
      "Dataset 40 âœ¨ Weighted Score (WS) for Event [51363â€“55870]: 0.3821\n",
      "Dataset 42 âœ¨ Weighted Score (WS) for Event [52303â€“53309]: 0.1929\n",
      "Dataset 45 âœ¨ Weighted Score (WS) for Event [52731â€“53738]: 0.1462\n",
      "Dataset 68 âœ¨ Weighted Score (WS) for Event [52063â€“54076]: 0.7275\n",
      "Dataset 72 âœ¨ Weighted Score (WS) for Event [52497â€“53505]: 0.0138\n",
      "Dataset 73 âœ¨ Weighted Score (WS) for Event [52745â€“53753]: 0.1481\n",
      "Dataset 84 âœ¨ Weighted Score (WS) for Event [52623â€“53627]: 0.3331\n",
      "\n",
      "ðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm A: 0.2496\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of datasets with their event start and end IDs\n",
    "datasets = [\n",
    "    (0, 52436, 54447),\n",
    "    (10, 52611, 53591),\n",
    "    (22, 51888, 52892),\n",
    "    (26, 52261, 53269),\n",
    "    (40, 51363, 55870),\n",
    "    (42, 52303, 53309),\n",
    "    (45, 52731, 53738),\n",
    "    (68, 52063, 54076),\n",
    "    (72, 52497, 53505),\n",
    "    (73, 52745, 53753),\n",
    "    (84, 52623, 53627),\n",
    "]\n",
    "\n",
    "def weight_function(relative_pos):\n",
    "    # Piecewise weight function as per author:\n",
    "    # weight = 1 for first half (relative_pos <= 0.5)\n",
    "    # weight decreases linearly from 1 to 0 in second half\n",
    "    if relative_pos <= 0.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 2 * (1 - relative_pos)  # linear decrease from 1 to 0 between 0.5 and 1\n",
    "\n",
    "ws_scores = []\n",
    "\n",
    "\n",
    "for dataset_id, event_start, event_end in datasets:\n",
    "    ground_truth_path = fr\"D:\\Master Thesis New Data Set\\CARE DATA SET\\CARE_To_Compare\\Wind Farm A\\Wind Farm A\\datasets\\{dataset_id}.csv\"\n",
    "    predicted_path = fr\"D:\\Master Thesis New Data Set\\Final Processed Dataset\\Wind Farm A\\{dataset_id}_WindFarm_A_predictions_lgb_smoothed.csv\"\n",
    "\n",
    "    is_abnormal_event = True  # all events are abnormal\n",
    "\n",
    "    # === Load data ===\n",
    "    gt_df = pd.read_csv(ground_truth_path, delimiter=';')\n",
    "    pred_df = pd.read_csv(predicted_path)\n",
    "\n",
    "    # Filter to prediction time frame only\n",
    "    gt_df = gt_df[gt_df['train_test'].str.lower() == 'prediction'].copy()\n",
    "    pred_df = pred_df[pred_df['train_test'].str.lower() == 'prediction'].copy()\n",
    "\n",
    "    # Set anomaly labels\n",
    "    gt_df['anomaly_label'] = 0\n",
    "    if is_abnormal_event:\n",
    "        event_mask = (gt_df['id'] >= event_start) & (gt_df['id'] <= event_end)\n",
    "        gt_df.loc[event_mask, 'anomaly_label'] = 1\n",
    "\n",
    "    # Merge with predictions\n",
    "    merged_df = pd.merge(\n",
    "        gt_df[['id', 'anomaly_label']],\n",
    "        pred_df[['id', 'predicted_status_type_binary_smooth']],\n",
    "        on='id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Convert predicted_status_type_id to binary labels: 1 = anomaly, 0 = normal\n",
    "    merged_df['pred_label'] = merged_df['predicted_status_type_binary_smooth'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    # Filter to event window\n",
    "    event_df = merged_df[(merged_df['id'] >= event_start) & (merged_df['id'] <= event_end)].copy()\n",
    "\n",
    "    # Calculate weights for all timestamps in the event window\n",
    "    event_df['relative_pos'] = (event_df['id'] - event_start) / (event_end - event_start)\n",
    "    event_df['weight'] = event_df['relative_pos'].apply(weight_function)\n",
    "\n",
    "    # WS numerator: sum of weights * predicted anomalies\n",
    "    numerator = (event_df['weight'] * event_df['pred_label']).sum()\n",
    "\n",
    "    # WS denominator: sum of weights (all timestamps in event window)\n",
    "    denominator = event_df['weight'].sum()\n",
    "\n",
    "    ws = numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    ws_scores.append(ws)\n",
    "\n",
    "    print(f\"Dataset {dataset_id} âœ¨ Weighted Score (WS) for Event [{event_start}â€“{event_end}]: {ws:.4f}\")\n",
    "\n",
    "# Average WS score\n",
    "average_ws_a = np.mean(ws_scores)\n",
    "print(f\"\\nðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm A: {average_ws_a:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03240085-df03-4176-8e65-ec6fb2006a1e",
   "metadata": {},
   "source": [
    "## Wind Farm B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac97c09-d9fe-4a80-97e0-e62e7e8e6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 7 âœ¨ Weighted Score (WS) for Event [52703â€“57167]: 0.2177\n",
      "Dataset 19 âœ¨ Weighted Score (WS) for Event [52673â€“55553]: 1.0000\n",
      "Dataset 27 âœ¨ Weighted Score (WS) for Event [52619â€“61403]: 0.2486\n",
      "Dataset 34 âœ¨ Weighted Score (WS) for Event [52531â€“55699]: 0.4286\n",
      "Dataset 53 âœ¨ Weighted Score (WS) for Event [52559â€“58606]: 0.7809\n",
      "Dataset 77 âœ¨ Weighted Score (WS) for Event [52991â€“61631]: 1.0000\n",
      "\n",
      "ðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm B: 0.6126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of datasets with their event start and end IDs\n",
    "datasets = [\n",
    "    (7, 52703, 57167),\n",
    "    (19, 52673, 55553),\n",
    "    (27, 52619, 61403),\n",
    "    (34, 52531, 55699),\n",
    "    (53, 52559, 58606),\n",
    "    (77, 52991, 61631),\n",
    "]\n",
    "\n",
    "def weight_function(relative_pos):\n",
    "    # Piecewise weight function as per author:\n",
    "    # weight = 1 for first half (relative_pos <= 0.5)\n",
    "    # weight decreases linearly from 1 to 0 in second half\n",
    "    if relative_pos <= 0.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 2 * (1 - relative_pos)  # linear decrease from 1 to 0 between 0.5 and 1\n",
    "\n",
    "ws_scores = []\n",
    "\n",
    "for dataset_id, event_start, event_end in datasets:\n",
    "    ground_truth_path = fr\"D:\\Master Thesis New Data Set\\CARE DATA SET\\CARE_To_Compare\\Wind Farm B\\Wind Farm B\\datasets\\{dataset_id}.csv\"\n",
    "    predicted_path = fr\"D:\\Master Thesis New Data Set\\Final Processed Dataset\\Wind Farm B\\{dataset_id}_WindFarm_B_predictions_lgb_smoothed.csv\"\n",
    "\n",
    "    is_abnormal_event = True  # all events are abnormal\n",
    "\n",
    "    # === Load data ===\n",
    "    gt_df = pd.read_csv(ground_truth_path, delimiter=';')\n",
    "    pred_df = pd.read_csv(predicted_path)\n",
    "\n",
    "    # Filter to prediction time frame only\n",
    "    gt_df = gt_df[gt_df['train_test'].str.lower() == 'prediction'].copy()\n",
    "    pred_df = pred_df[pred_df['train_test'].str.lower() == 'prediction'].copy()\n",
    "\n",
    "    # Set anomaly labels\n",
    "    gt_df['anomaly_label'] = 0\n",
    "    if is_abnormal_event:\n",
    "        event_mask = (gt_df['id'] >= event_start) & (gt_df['id'] <= event_end)\n",
    "        gt_df.loc[event_mask, 'anomaly_label'] = 1\n",
    "\n",
    "    # Merge with predictions\n",
    "    merged_df = pd.merge(\n",
    "        gt_df[['id', 'anomaly_label']],\n",
    "        pred_df[['id', 'predicted_status_type_binary_smooth']],\n",
    "        on='id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Convert predicted_status_type_id to binary labels: 1 = anomaly, 0 = normal\n",
    "    merged_df['pred_label'] = merged_df['predicted_status_type_binary_smooth'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    # Filter to event window\n",
    "    event_df = merged_df[(merged_df['id'] >= event_start) & (merged_df['id'] <= event_end)].copy()\n",
    "\n",
    "    # Calculate weights for all timestamps in the event window\n",
    "    event_df['relative_pos'] = (event_df['id'] - event_start) / (event_end - event_start)\n",
    "    event_df['weight'] = event_df['relative_pos'].apply(weight_function)\n",
    "\n",
    "    # WS numerator: sum of weights * predicted anomalies\n",
    "    numerator = (event_df['weight'] * event_df['pred_label']).sum()\n",
    "\n",
    "    # WS denominator: sum of weights (all timestamps in event window)\n",
    "    denominator = event_df['weight'].sum()\n",
    "\n",
    "    ws = numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    ws_scores.append(ws)\n",
    "\n",
    "    print(f\"Dataset {dataset_id} âœ¨ Weighted Score (WS) for Event [{event_start}â€“{event_end}]: {ws:.4f}\")\n",
    "\n",
    "# Average WS score\n",
    "average_ws_b = np.mean(ws_scores)\n",
    "print(f\"\\nðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm B: {average_ws_b:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ccdb4-13f0-4fa9-a8fe-7a4973c6d37a",
   "metadata": {},
   "source": [
    "## Wind Farm C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6902f30b-a06c-4b62-8d15-e6dcc582906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 4 âœ¨ Weighted Score (WS) for Event [52992â€“55728]: 0.1132\n",
      "Dataset 5 âœ¨ Weighted Score (WS) for Event [52272â€“52794]: 0.6587\n",
      "Dataset 9 âœ¨ Weighted Score (WS) for Event [52992â€“56028]: 0.1553\n",
      "Dataset 11 âœ¨ Weighted Score (WS) for Event [52416â€“55572]: 1.0000\n",
      "Dataset 12 âœ¨ Weighted Score (WS) for Event [52560â€“55818]: 1.0000\n",
      "Dataset 15 âœ¨ Weighted Score (WS) for Event [51984â€“54432]: 0.2220\n",
      "Dataset 16 âœ¨ Weighted Score (WS) for Event [51264â€“53423]: 0.8610\n",
      "Dataset 18 âœ¨ Weighted Score (WS) for Event [51408â€“51983]: 0.2541\n",
      "Dataset 28 âœ¨ Weighted Score (WS) for Event [52704â€“55629]: 0.2703\n",
      "Dataset 30 âœ¨ Weighted Score (WS) for Event [52560â€“55822]: 0.8136\n",
      "Dataset 31 âœ¨ Weighted Score (WS) for Event [52848â€“53868]: 0.6175\n",
      "Dataset 33 âœ¨ Weighted Score (WS) for Event [52848â€“55728]: 0.0764\n",
      "Dataset 35 âœ¨ Weighted Score (WS) for Event [51696â€“52614]: 0.1060\n",
      "Dataset 39 âœ¨ Weighted Score (WS) for Event [52848â€“53582]: 0.6111\n",
      "Dataset 44 âœ¨ Weighted Score (WS) for Event [52704â€“62138]: 0.2294\n",
      "Dataset 47 âœ¨ Weighted Score (WS) for Event [52416â€“53128]: 0.2154\n",
      "Dataset 49 âœ¨ Weighted Score (WS) for Event [51840â€“52437]: 0.2856\n",
      "Dataset 55 âœ¨ Weighted Score (WS) for Event [52848â€“55320]: 0.5243\n",
      "Dataset 66 âœ¨ Weighted Score (WS) for Event [51696â€“52638]: 0.9590\n",
      "Dataset 67 âœ¨ Weighted Score (WS) for Event [52704â€“61056]: 0.3234\n",
      "Dataset 70 âœ¨ Weighted Score (WS) for Event [52560â€“55461]: 0.4791\n",
      "Dataset 76 âœ¨ Weighted Score (WS) for Event [51552â€“51797]: 0.0000\n",
      "Dataset 78 âœ¨ Weighted Score (WS) for Event [52560â€“52857]: 0.5241\n",
      "Dataset 79 âœ¨ Weighted Score (WS) for Event [52704â€“52992]: 0.0000\n",
      "Dataset 81 âœ¨ Weighted Score (WS) for Event [52704â€“53067]: 0.0000\n",
      "Dataset 90 âœ¨ Weighted Score (WS) for Event [52848â€“54591]: 0.2277\n",
      "Dataset 91 âœ¨ Weighted Score (WS) for Event [52704â€“55599]: 0.2305\n",
      "\n",
      "ðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm C: 0.3984\n"
     ]
    }
   ],
   "source": [
    "# List of datasets with their event start and end IDs\n",
    "datasets = [\n",
    "    (4, 52992, 55728),\n",
    "    (5, 52272, 52794),\n",
    "    (9, 52992, 56028),\n",
    "    (11, 52416, 55572),\n",
    "    (12, 52560, 55818),\n",
    "    (15, 51984, 54432),\n",
    "    (16, 51264, 53423),\n",
    "    (18, 51408, 51983),\n",
    "    (28, 52704, 55629),\n",
    "    (30, 52560, 55822),\n",
    "    (31, 52848, 53868),\n",
    "    (33, 52848, 55728),\n",
    "    (35, 51696, 52614),\n",
    "    (39, 52848, 53582),\n",
    "    (44, 52704, 62138),\n",
    "    (47, 52416, 53128),\n",
    "    (49, 51840, 52437),\n",
    "    (55, 52848, 55320),\n",
    "    (66, 51696, 52638),\n",
    "    (67, 52704, 61056),\n",
    "    (70, 52560, 55461),\n",
    "    (76, 51552, 51797),\n",
    "    (78, 52560, 52857),\n",
    "    (79, 52704, 52992),\n",
    "    (81, 52704, 53067),\n",
    "    (90, 52848, 54591),\n",
    "    (91, 52704, 55599),\n",
    "]\n",
    "\n",
    "def weight_function(relative_pos):\n",
    "    # Piecewise weight function as per author:\n",
    "    # weight = 1 for first half (relative_pos <= 0.5)\n",
    "    # weight decreases linearly from 1 to 0 in second half\n",
    "    if relative_pos <= 0.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 2 * (1 - relative_pos)  # linear decrease from 1 to 0 between 0.5 and 1\n",
    "\n",
    "ws_scores = []\n",
    "\n",
    "for dataset_id, event_start, event_end in datasets:\n",
    "    ground_truth_path = fr\"D:\\Master Thesis New Data Set\\CARE DATA SET\\CARE_To_Compare\\Wind Farm C\\Wind Farm C\\datasets\\{dataset_id}.csv\"\n",
    "    predicted_path = fr\"D:\\Master Thesis New Data Set\\Final Processed Dataset\\Wind Farm C\\{dataset_id}_WindFarm_C_predictions_lgb_smoothed.csv\"\n",
    "\n",
    "    is_abnormal_event = True  # all events are abnormal\n",
    "\n",
    "    # === Load data ===\n",
    "    gt_df = pd.read_csv(ground_truth_path, delimiter=';')\n",
    "    pred_df = pd.read_csv(predicted_path)\n",
    "\n",
    "    # Filter to prediction time frame only\n",
    "    gt_df = gt_df[gt_df['train_test'].str.lower() == 'prediction'].copy()\n",
    "    pred_df = pred_df[pred_df['train_test'].str.lower() == 'prediction'].copy()\n",
    "\n",
    "    # Set anomaly labels\n",
    "    gt_df['anomaly_label'] = 0\n",
    "    if is_abnormal_event:\n",
    "        event_mask = (gt_df['id'] >= event_start) & (gt_df['id'] <= event_end)\n",
    "        gt_df.loc[event_mask, 'anomaly_label'] = 1\n",
    "\n",
    "    # Merge with predictions\n",
    "    merged_df = pd.merge(\n",
    "        gt_df[['id', 'anomaly_label']],\n",
    "        pred_df[['id', 'predicted_status_type_binary_smooth']],\n",
    "        on='id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Convert predicted_status_type_id to binary labels: 1 = anomaly, 0 = normal\n",
    "    merged_df['pred_label'] = merged_df['predicted_status_type_binary_smooth'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    # Filter to event window\n",
    "    event_df = merged_df[(merged_df['id'] >= event_start) & (merged_df['id'] <= event_end)].copy()\n",
    "\n",
    "    # Calculate weights for all timestamps in the event window\n",
    "    event_df['relative_pos'] = (event_df['id'] - event_start) / (event_end - event_start)\n",
    "    event_df['weight'] = event_df['relative_pos'].apply(weight_function)\n",
    "\n",
    "    # WS numerator: sum of weights * predicted anomalies\n",
    "    numerator = (event_df['weight'] * event_df['pred_label']).sum()\n",
    "\n",
    "    # WS denominator: sum of weights (all timestamps in event window)\n",
    "    denominator = event_df['weight'].sum()\n",
    "\n",
    "    ws = numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    ws_scores.append(ws)\n",
    "\n",
    "    print(f\"Dataset {dataset_id} âœ¨ Weighted Score (WS) for Event [{event_start}â€“{event_end}]: {ws:.4f}\")\n",
    "\n",
    "# Average WS score\n",
    "average_ws_c = np.mean(ws_scores)\n",
    "print(f\"\\nðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm C: {average_ws_c:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c422f-528f-451c-af48-ae446d361cc9",
   "metadata": {},
   "source": [
    "## Overall Earliness for All Wind Farms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4296ae6-d59e-4e12-9d4a-d0a8a3a3632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm C: 0.3904\n"
     ]
    }
   ],
   "source": [
    "average_ws =  ((average_ws_a * 11) + (average_ws_b * 6) + (average_ws_c * 27))/44\n",
    "print(f\"\\nðŸ”¥ Average Weighted Score (WS) across all datasets of wind farm C: {average_ws:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
