{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887b0b6b-370a-4597-8cf0-83ee1e8bc857",
   "metadata": {},
   "source": [
    " ## Light GBM and Smoothing Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7de338-a368-49d3-80f2-c6c15f01e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49157\\miniconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py:31: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== Processing Dataset 2 ==================\n",
      "✅ Accuracy: 0.8885\n",
      "✅ Precision: 0.5200\n",
      "✅ Recall: 0.0524\n",
      "✅ F1 Score: 0.0952\n",
      "\n",
      "================== Processing Dataset 7 ==================\n",
      "✅ Accuracy: 0.8531\n",
      "✅ Precision: 0.8601\n",
      "✅ Recall: 0.6439\n",
      "✅ F1 Score: 0.7365\n",
      "\n",
      "================== Processing Dataset 19 ==================\n",
      "✅ Accuracy: 0.9191\n",
      "✅ Precision: 0.9277\n",
      "✅ Recall: 0.7523\n",
      "✅ F1 Score: 0.8308\n",
      "\n",
      "================== Processing Dataset 21 ==================\n",
      "✅ Accuracy: 0.8211\n",
      "✅ Precision: 0.9211\n",
      "✅ Recall: 0.3889\n",
      "✅ F1 Score: 0.5469\n",
      "\n",
      "================== Processing Dataset 23 ==================\n",
      "✅ Accuracy: 0.7786\n",
      "✅ Precision: 0.8654\n",
      "✅ Recall: 0.4845\n",
      "✅ F1 Score: 0.6212\n",
      "\n",
      "================== Processing Dataset 27 ==================\n",
      "✅ Accuracy: 0.8600\n",
      "✅ Precision: 0.9376\n",
      "✅ Recall: 0.7076\n",
      "✅ F1 Score: 0.8065\n",
      "\n",
      "================== Processing Dataset 34 ==================\n",
      "✅ Accuracy: 0.8113\n",
      "✅ Precision: 0.7922\n",
      "✅ Recall: 0.6198\n",
      "✅ F1 Score: 0.6955\n",
      "\n",
      "================== Processing Dataset 52 ==================\n",
      "✅ Accuracy: 0.8173\n",
      "✅ Precision: 0.7746\n",
      "✅ Recall: 0.3884\n",
      "✅ F1 Score: 0.5174\n",
      "\n",
      "================== Processing Dataset 53 ==================\n",
      "✅ Accuracy: 0.7202\n",
      "✅ Precision: 0.7733\n",
      "✅ Recall: 0.8372\n",
      "✅ F1 Score: 0.8040\n",
      "\n",
      "================== Processing Dataset 74 ==================\n",
      "✅ Accuracy: 0.8962\n",
      "✅ Precision: 0.9065\n",
      "✅ Recall: 0.9071\n",
      "✅ F1 Score: 0.9068\n",
      "\n",
      "================== Processing Dataset 77 ==================\n",
      "✅ Accuracy: 0.7185\n",
      "✅ Precision: 0.6746\n",
      "✅ Recall: 0.6805\n",
      "✅ F1 Score: 0.6775\n",
      "\n",
      "================== Processing Dataset 82 ==================\n",
      "✅ Accuracy: 0.8481\n",
      "✅ Precision: 0.9330\n",
      "✅ Recall: 0.7073\n",
      "✅ F1 Score: 0.8046\n",
      "\n",
      "================== Processing Dataset 83 ==================\n",
      "✅ Accuracy: 0.7057\n",
      "✅ Precision: 0.6206\n",
      "✅ Recall: 0.4233\n",
      "✅ F1 Score: 0.5033\n",
      "\n",
      "================== Processing Dataset 86 ==================\n",
      "✅ Accuracy: 0.8924\n",
      "✅ Precision: 0.9613\n",
      "✅ Recall: 0.6146\n",
      "✅ F1 Score: 0.7498\n",
      "\n",
      "================== Processing Dataset 87 ==================\n",
      "✅ Accuracy: 0.8453\n",
      "✅ Precision: 0.9013\n",
      "✅ Recall: 0.5732\n",
      "✅ F1 Score: 0.7008\n",
      "\n",
      "================== Final Summary Across All Datasets ==================\n",
      "Mean Accuracy: 0.8250\n",
      "Mean Precision: 0.8246\n",
      "Mean Recall: 0.5854\n",
      "Mean F1 Score: 0.6665\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def adjust_prediction_probabilities(y_pred_probs, recent_preds, confidence_threshold=0.9996, max_weight=0.65, min_consecutive=2, weight_per_consecutive=0.17):\n",
    "    \"\"\"\n",
    "    Adjust predicted probabilities based on recent consecutive predictions.\n",
    "\n",
    "    Args:\n",
    "        y_pred_probs: numpy array of shape (2,), predicted probs for classes [0,1]\n",
    "        recent_preds: list of recent predictions (0/1)\n",
    "        confidence_threshold: no adjustment if model confidence > threshold\n",
    "        max_weight: max total weight to add\n",
    "        min_consecutive: minimum run length to start adjustment\n",
    "        weight_per_consecutive: weight increment per consecutive prediction\n",
    "\n",
    "    Returns:\n",
    "        adjusted_probs: numpy array of adjusted probabilities summing to 1\n",
    "    \"\"\"\n",
    "    if not recent_preds or len(recent_preds) < min_consecutive:\n",
    "        return y_pred_probs\n",
    "\n",
    "    last_class = recent_preds[-1]\n",
    "    count = 1\n",
    "    for i in range(len(recent_preds) - 2, -1, -1):\n",
    "        if recent_preds[i] == last_class:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if count < min_consecutive:\n",
    "        return y_pred_probs\n",
    "\n",
    "    top_class = np.argmax(y_pred_probs)\n",
    "    top_confidence = y_pred_probs[top_class]\n",
    "\n",
    "    # Skip adjustment if model already confident\n",
    "    if top_confidence >= confidence_threshold:\n",
    "        return y_pred_probs\n",
    "\n",
    "    # Calculate weight based on consecutive count\n",
    "    weight = min((count - 1) * weight_per_consecutive, max_weight)\n",
    "\n",
    "    adjusted_probs = y_pred_probs.copy()\n",
    "    adjusted_probs[last_class] += weight\n",
    "    other_class = 1 - last_class\n",
    "    adjusted_probs[other_class] -= weight\n",
    "\n",
    "    # Clip between 0 and 1, normalize to sum 1\n",
    "    adjusted_probs = np.clip(adjusted_probs, 0, 1)\n",
    "    adjusted_probs /= adjusted_probs.sum()\n",
    "\n",
    "    return adjusted_probs\n",
    "\n",
    "dataset_ids = [2,7,19,21,23,27,34,52,53,74,77,82,83,86,87]\n",
    "\n",
    "save_dir = r\"D:\\Master Thesis New Data Set\\Analysis\\Model Selection\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i in dataset_ids:\n",
    "    print(f\"\\n================== Processing Dataset {i} ==================\")\n",
    "\n",
    "    df = pd.read_csv(rf\"D:\\Master Thesis New Data Set\\Status Reassignment Dataset\\Wind Farm B\\{i}_WindFarm_B.csv\", delimiter=',') ## Add Power Curved Status Reassigned Data here\n",
    "\n",
    "    df['train_test'] = df['train_test'].astype(str).str.strip().str.lower()\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "    df['hour'] = df['time_stamp'].dt.hour\n",
    "    df['dayofweek'] = df['time_stamp'].dt.dayofweek\n",
    "    df['month'] = df['time_stamp'].dt.month\n",
    "\n",
    "    df['status_type_binary'] = df['status_type_id'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "    train_df = df[df['train_test'] == 'train']\n",
    "    predict_df = df[df['train_test'] == 'prediction']\n",
    "\n",
    "    cols_to_remove = ['status_type_id', 'status_type_binary', 'time_stamp', 'id', 'asset_id', 'train_test']\n",
    "    feature_cols = [col for col in df.columns if col not in cols_to_remove]\n",
    "\n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    X_predict = predict_df[feature_cols].copy()\n",
    "    y_train = train_df['status_type_binary']\n",
    "\n",
    "    label_encoders = {}\n",
    "    for col in X_train.select_dtypes(include=['object', 'category']).columns:\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        X_predict[col] = le.transform(X_predict[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    clf = lgb.train(params, lgb_train, num_boost_round=100)\n",
    "\n",
    "    recent_preds = []\n",
    "    y_pred_binary_smooth = []\n",
    "\n",
    "    for idx in range(len(X_predict)):\n",
    "        row = X_predict.iloc[idx:idx+1]\n",
    "        # Get probs for classes [0,1] from LightGBM\n",
    "        prob_1 = clf.predict(row)[0]\n",
    "        prob_0 = 1 - prob_1\n",
    "        probs = np.array([prob_0, prob_1])\n",
    "\n",
    "        # Adjust probabilities based on recent preds\n",
    "        adjusted_probs = adjust_prediction_probabilities(probs, recent_preds)\n",
    "\n",
    "        # Final prediction\n",
    "        pred_class = int(np.argmax(adjusted_probs))\n",
    "\n",
    "        recent_preds.append(pred_class)\n",
    "        if len(recent_preds) > 5:\n",
    "            recent_preds.pop(0)\n",
    "\n",
    "        y_pred_binary_smooth.append(pred_class)\n",
    "\n",
    "    predict_df = predict_df.copy()\n",
    "    predict_df['predicted_status_type_binary'] = (clf.predict(X_predict) >= 0.5).astype(int)\n",
    "    predict_df['predicted_status_type_binary_smooth'] = y_pred_binary_smooth\n",
    "\n",
    "    pred_file = os.path.join(save_dir, f\"{i}_WindFarm_B_predictions_lgb_smoothed.csv\")\n",
    "    predict_df.to_csv(pred_file, index=False)\n",
    "\n",
    "    actual_test_df = df[df['train_test'] == 'prediction'][['id', 'status_type_binary']]\n",
    "    comparison_df = pd.merge(predict_df, actual_test_df, on='id', suffixes=('_predicted', '_actual'))\n",
    "\n",
    "    y_true = comparison_df['status_type_binary_actual']\n",
    "    y_pred_final = comparison_df['predicted_status_type_binary_smooth']\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred_final)\n",
    "    precision = precision_score(y_true, y_pred_final, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_final, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_final, zero_division=0)\n",
    "\n",
    "    all_results.append({\n",
    "        'Dataset': i,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1\n",
    "    })\n",
    "\n",
    "    print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"✅ Precision: {precision:.4f}\")\n",
    "    print(f\"✅ Recall: {recall:.4f}\")\n",
    "    print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "mean_results = results_df.mean(numeric_only=True)\n",
    "\n",
    "print(\"\\n================== Final Summary Across All Datasets ==================\")\n",
    "print(f\"Mean Accuracy: {mean_results['Accuracy']:.4f}\")\n",
    "print(f\"Mean Precision: {mean_results['Precision']:.4f}\")\n",
    "print(f\"Mean Recall: {mean_results['Recall']:.4f}\")\n",
    "print(f\"Mean F1 Score: {mean_results['F1_Score']:.4f}\")\n",
    "\n",
    "summary_file = os.path.join(save_dir, \"lightgbm_evaluation_summary_smoothed.xlsx\")\n",
    "results_df.to_excel(summary_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a2ec7-83f0-4d11-941c-ab73a99aaf07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
